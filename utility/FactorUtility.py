# -*-coding:utf-8-*-
# @Time:   2021/4/14 13:43
# @Author: FC
# @Email:  18817289038@163.com

import time
import numpy as np
import pandas as pd
import datetime as dt
import statsmodels.api as sm

from scipy import stats
from sklearn.decomposition import PCA
from typing import Dict, Any, List, Union
from statsmodels.tsa.arima_model import ARMA
from itertools import combinations_with_replacement
from sklearn.metrics import normalized_mutual_info_score as nor_mi

from utility.Optimization import MaxOptModel
from constant import (
    KeyName as KN,
    PriceVolumeName as PVN,
    SpecialName as SN
)


# å› å­å¤„ç†æ–¹æ³•åˆé›†
class MethodSets(object):
    # æ–¹æ³•å‚æ•°å¿…é¡»è¢«ç»§æ‰¿å¤å†™

    methodProcess = {
        "RO": {"method": "", "p": {}},  # å¼‚å¸¸å€¼å¤„ç†
        "Neu": {"method": "", "p": {}},  # ä¸­æ€§åŒ–å¤„ç†
        "Sta": {"method": "", "p": {}},  # æ ‡å‡†åŒ–å¤„ç†

        "Cor": {"method": "", "p": {}},  # ç›¸å…³æ€§è®¡ç®—
        "Syn": {"method": "", "p": {}},  # å› å­åˆæˆ

        "Ret": {"method": "", "p": {}},  # æ”¶ç›Šé¢„æµ‹æ–¹æ³•
        "Risk": {"method": "", "p": {}},  # é£Žé™©åˆ†æžæ–¹æ³•
    }

    def __init__(self):
        self.Cor = Correlation()
        self.Syn = DataSynthesis()
        self.RO = RemoveOutlier()
        self.Neu = Neutralization()
        self.Sta = Standardization()

    # æ›´æ–°å‚æ•°
    def set_params(self, **kwargs):
        """

        Parameters
        ----------
        Returns
        -------
        å¯¹äºŽå› å­å¤„ç†æ–¹æ³•è®¾ç½®å› å­å‚æ•°
        """
        for paramName, paramValue in kwargs.items():
            setattr(self, paramName, paramValue)

    def processSingle(self,
                      data: Union[pd.DataFrame, pd.Series],
                      methodN: str,
                      **kwargs) -> Any:
        """
        å•ä¸€å¤„ç†æ–¹æ³•
        Parameters
        ----------
        data :
        methodN : æ–¹æ³•å
        kwargs :

        Returns
        -------

        """
        value = getattr(self, methodN).process(data=data,
                                               method=self.methodProcess[methodN]['method'],
                                               **self.methodProcess[methodN]['p'],
                                               **kwargs)
        return value

    def processSeq(self,
                   data: pd.DataFrame,
                   methodN: List[str],
                   dataName: str,
                   **kwargs):
        """
        è¿žç»­å¤„ç†
        Parameters
        ----------
        data :
        methodN : æ–¹æ³•ålistï¼ŒæŒ‰é¡ºåºæ‰§è¡Œ
        dataName :
        kwargs :

        Returns
        -------

        """
        for M in methodN:
            if self.methodProcess[M]['method'] != "":
                value = getattr(self, M).process(data=data,
                                                 method=self.methodProcess[M]['method'],
                                                 dataName=dataName,
                                                 **self.methodProcess[M]['p'],
                                                 **kwargs)
                data[dataName] = value


# åŽ»æžå€¼
class RemoveOutlier(object):
    dataName = ''

    def process(self,
                data: pd.DataFrame,
                dataName: str,
                method='before_after_3%',
                **kwargs) -> pd.Series(float):

        self.dataName = dataName

        method_dict = {
            "before_after_3%": self.before_after_n,
            "before_after_3sigma": self.before_after_3sigma,
            "mad": self.mad
        }
        if method is None:
            return data
        else:
            res = method_dict[method](data, **kwargs)
            return res

    """åŽ»æžå€¼"""

    # å‰åŽ3%
    def before_after_n(self,
                       data: pd.DataFrame,
                       n: int = 3,
                       **kwargs) -> pd.Series(float):
        data_df = data[self.dataName].unstack()
        threshold_down, threshold_up = data_df.quantile(n / 100, axis=1), data_df.quantile(1 - n / 100, axis=1)
        res = data_df.clip(threshold_down, threshold_up, axis=0).stack()
        return res

    # 3å€æ ‡å‡†å·®å¤–
    def before_after_3sigma(self,
                            data: pd.DataFrame,
                            **kwargs) -> pd.Series(float):
        data_df = data[self.dataName].unstack()
        miu, sigma = data_df.mean(axis=1), data_df.std(axis=1)
        threshold_down, threshold_up = miu - 3 * sigma, miu + 3 * sigma
        res = data_df.clip(threshold_down, threshold_up, axis=0).stack()
        return res

    # ç»å¯¹ä¸­ä½åå·®æ³•
    def mad(self,
            data: pd.DataFrame,
            **kwargs) -> pd.Series(float):
        data_df = data[self.dataName].unstack()
        median = data_df.median(axis=1)
        MAD = data_df.sub(median, axis=0).abs().median(axis=1)
        threshold_up, threshold_down = median + 3 * 1.483 * MAD, median - 3 * 1.483 * MAD
        res = data_df.clip(threshold_down, threshold_up, axis=0).stack()
        return res


# ä¸­æ€§åŒ–
class Neutralization(object):
    dataName = ''

    def process(self,
                data: pd.Series,
                dataName: str,
                mvName: str = PVN.LIQ_MV.value,
                indName: str = SN.INDUSTRY_FLAG.value,
                method: str = 'industry+mv',
                **kwargs) -> pd.Series(float):
        """
        è‹¥åŒæ—¶çº³å…¥è¡Œä¸šå› å­å’Œå¸‚å€¼å› å­éœ€è¦åŠ ä¸Šæˆªè·é¡¹ï¼Œè‹¥ä»…çº³å…¥è¡Œä¸šå› å­åˆ™å›žå½’æ–¹ç¨‹ä¸å¸¦æˆªè·é¡¹ï¼
        :param data: å› å­æ•°æ®
        :param dataName: å› å­æ•°æ®
        :param mvName: å¸‚å€¼åç§°
        :param indName: è¡Œä¸šæŒ‡æ•°åç§°
        :param method: ä¸­å¿ƒåŒ–æ–¹æ³•
        :return: å‰”é™¤è¡Œä¸šå› ç´ å’Œå¸‚å€¼å› ç´ åŽçš„å› å­

        Args:
            indName ():
            mvName ():

        Parameters
        ----------
        factName :
        """
        self.dataName = dataName

        colName = [self.dataName]
        # read mv and industry data
        if 'mv' in method:
            colName.append(mvName)

        if 'industry' in method:
            colName.append(indName)

        # remove Nan
        dataNew = data[colName].dropna(how='any').copy()
        # neutralization
        res = dataNew.groupby(KN.TRADE_DATE.value, group_keys=False).apply(self.reg)
        return res

    # regression
    def reg(self, data: pd.DataFrame) -> pd.Series(float):
        """ï¼ï¼ï¼ä¸æŽ’åºå›žå½’ç»“æžœä¼šä¸ä¸€æ ·ï¼ï¼ï¼"""
        dataSub = data.sort_index()

        X = pd.get_dummies(dataSub.loc[:, dataSub.columns != self.dataName], columns=[SN.INDUSTRY_FLAG.value])
        Y = dataSub[self.dataName]
        reg = np.linalg.lstsq(X, Y)
        factNeu = Y - (reg[0] * X).sum(axis=1)
        return factNeu


# æ ‡å‡†åŒ–
class Standardization(object):
    dataName = ''

    def process(self,
                data: pd.DataFrame,
                dataName: str,
                method='z_score',
                **kwargs) -> pd.Series(float):
        self.dataName = dataName

        method_dict = {"range01": self.range01,
                       "z_score": self.z_score,
                       "mv": self.market_value_weighting
                       }

        if method is None:
            return data

        res = method_dict[method](data, **kwargs)

        return res

    """æ ‡å‡†åŒ–"""

    # æ ‡å‡†åˆ†æ•°æ³•
    def z_score(self,
                data: pd.DataFrame,
                **kwargs):
        """
        :param data:
        :return:
        """
        data_df = data[self.dataName].unstack()
        miu, sigma = data_df.mean(axis=1), data_df.std(axis=1)
        stand = data_df.sub(miu, axis=0).div(sigma, axis=0).stack()
        return stand

    def range01(self,
                data: pd.DataFrame,
                **kwargs):
        data_df = data[self.dataName]
        denominator, numerator = data_df.max(axis=1) - data_df.min(axis=1), data_df.sub(data_df.min(axis=1), axis=0)
        result = numerator.div(denominator, axis=0).stack()
        return result

    # å¸‚å€¼åŠ æƒæ ‡å‡†åŒ–
    def market_value_weighting(self,
                               data: pd.DataFrame,
                               mvName: str = PVN.LIQ_MV.value,
                               **kwargs) -> pd.Series(float):
        data_df = data[[self.dataName, mvName]].dropna(how='any')
        dataFact = data_df[self.dataName].unstack()
        dataMv = data_df[mvName].unstack()

        miu, std = (dataFact * dataMv).div(dataMv.sum(axis=1), axis=0).sum(axis=1), dataFact.std(axis=1)
        res = dataFact.sub(miu, axis=0).div(std, axis=0).stack()

        return res


# ç›¸å…³æ€§æ£€éªŒ
class Correlation(object):

    def process(self,
                data: pd.DataFrame,
                method='LinCor',
                **kwargs) -> pd.Series(float):

        method_dict = {"LinCor": self.correlation,
                       "MI": self.MI,
                       }

        if method is None:
            return data

        res = method_dict[method](data, **kwargs)

        return res

    # çº¿æ€§ç›¸å…³æ€§æ£€éªŒ
    def correlation(self,
                    data: pd.DataFrame,
                    corName: str,
                    **kwargs) -> Dict[str, Any]:
        """
        æ¯æœŸæœ‰æ•ˆæ•°æ®è¿‡å°‘ä¸è®¡ç®—ç›¸å…³æ€§
        Parameters
        ----------
        data :
        corName :
        kwargs :

        Returns
        -------

        """
        dataSub = data.dropna()

        df_cor = dataSub.groupby(KN.TRADE_DATE.value).corr(method=corName)

        cor_GroupBy = df_cor.groupby(pd.Grouper(level=-1))
        cor_dict = {
            "corMean": cor_GroupBy.apply(lambda x: x.abs().mean().round(4)),
            "corMedian": cor_GroupBy.apply(lambda x: x.abs().median().round(4)),
            "corStd": cor_GroupBy.apply(lambda x: x.abs().std().round(4)),
            "corTtest_0_6": cor_GroupBy.apply(lambda x: pd.Series(stats.ttest_1samp(abs(x), 0.6)[0],
                                                                  index=x.columns).round(4)),
        }

        for corValue in cor_dict.values():
            corValue.replace(np.inf, np.nan, inplace=True)

        return cor_dict

    # éžçº¿æ€§ç›¸å…³æ€§æ£€éªŒ
    def MI(self, data: pd.DataFrame, **kwargs):
        dataSub = data.dropna()
        dataNames = dataSub.columns
        df_mi = pd.DataFrame(columns=dataNames, index=dataNames)

        iters = combinations_with_replacement(dataNames, 2)
        for ite_ in iters:
            df_mi.loc[ite_[0], ite_[1]] = nor_mi(dataSub[ite_[0]], dataSub[ite_[1]])


# å› å­åˆæˆæ–¹æ³•
class DataSynthesis(object):

    def __init__(self):
        self.opt = MaxOptModel()

    # å› å­å¤åˆ
    def process(self,
                data: pd.DataFrame,
                weight: pd.DataFrame,
                method: str = 'RetWeight',
                **kwargs) -> pd.DataFrame:
        """
        éƒ¨åˆ†æƒé‡ä¼šç”¨åˆ°æœªæ¥æ•°æ®ï¼Œæ‰€ä»¥éœ€è¦å¯¹æƒé‡è¿›è¡Œå¹³ç§»ä¸Žç›¸åº”çš„å› å­å€¼è¿›è¡ŒåŒ¹é…
        Parameters
        ----------
        data : å› å­é›†
        weight :å› å­æƒé‡
        method : å› å­åˆæˆæ–¹æ³•
        kwargs :

        Returns
        -------

        """

        # factor direction
        factDir = np.sign(weight.mean())

        # switch the factor directionï¼Œand turn the weight to positive
        factNew = data.mul(factDir, level=0).dropna(how='all')
        weightNew = weight.abs()

        method_dict = {"RetWeight": self.ret_weighted,
                       "OPT": self.MAX_IC_IR,
                       "PCA": self.PCA,
                       "Cluster": self.cluster
                       }

        if method is None:
            return data

        res = method_dict[method](fact=factNew, weight=weightNew, **kwargs)
        return res

    """å› å­åˆæˆ"""

    # åŠ æƒæ³•
    def ret_weighted(self,
                     fact: pd.DataFrame,
                     weight: pd.DataFrame,
                     weightAlgo: str = 'IC',
                     meanType: str = 'Equal',
                     **kwargs) -> pd.Series(float):
        """
        æŸè‚¡ç¥¨å› å­å€¼ä¸ºNanï¼Œåˆ™å°†å‰©ä½™å› å­å€¼è¿›è¡ŒåŠ æƒ
        é»˜è®¤é‡‡ç”¨ç­‰æƒåŠ æƒ
        Parameters
        ----------
        weight :
        fact :
        weightAlgo : RetMean: åŽ†å²æ”¶ç›ŠçŽ‡å‡å€¼ï¼Œ HalfTime: åŽ†å²æ”¶ç›ŠçŽ‡åŠè¡°åŠ æƒ
        meanType :
        kwargs :

        Returns
        -------

        """

        # TODO æµ‹è¯•
        if weightAlgo in ['IC', 'Ret']:
            # ç”Ÿæˆæƒé‡
            factW = self.mean_weight(weight, meanType)
            # å¯¹éžç©ºéƒ¨åˆ†è¿›è¡ŒåŠ æƒ
            fact_comp = self.weighted(fact, factW.values)
        elif weightAlgo == 'IC_IR':
            IC_Mean = self.mean_weight(weight, meanType)
            IC_Std = self.std_weight(weight, meanType)
            factW = IC_Mean / IC_Std

            fact_comp = self.weighted(fact, factW.values)
        else:
            fact_comp = fact.mean(axis=1)
        return fact_comp

    def MAX_IC_IR(self,
                  fact: pd.DataFrame,
                  weight: pd.DataFrame,
                  weightAlgo='IC_IR') -> pd.Series(float):

        # TODO  åŽ‹ç¼©çŸ©é˜µä¼°è®¡åæ–¹å·®
        self.opt.clear()
        # è®¾ç½®ä¼˜åŒ–æ–¹ç¨‹ç»„
        self.opt.obj_func = self.opt.object_func3
        self.opt.limit.append(self.opt.constraint())  # æƒé‡å’Œä¸º1
        self.opt.bonds = ((0, 1),) * fact.shape[1]  # æƒé‡å¤§äºŽ0

        weight_mean = np.array(weight.mean())
        if weightAlgo == 'IC':
            weight_cov = np.array(fact.cov())
        else:
            weight_cov = np.array(weight.cov())
        optParams = {
            "data_mean": weight_mean,
            "data_cov": weight_cov,
        }
        self.opt.set_params(**optParams)

        weightOpt = self.opt.solve()

        fact_comp = self.weighted(fact, weightOpt.x)
        return fact_comp

    def PCA(self,
            fact: pd.DataFrame,
            **kwargs):

        w_list = []
        for i in range(rp, fact.shape[0] + 1):
            df_ = fact.iloc[i - self.rp: i, :]

            pca = PCA(n_components=1)
            pca.fit(np.array(df_))
            weight = pca.components_[0]
            w_s = pd.Series(data=weight, index=df_.columns, name=df_.index[-1])
            w_list.append(w_s)
        w_df = pd.DataFrame(w_list)

        fact_comp = fact.mul(w_df).sum(axis=1)
        fact_comp.name = fact_comp

        return fact_comp

    def cluster(self,
                fact: pd.DataFrame):
        pass

    # æ­£äº¤åŒ–
    @staticmethod
    def orthogonal(factor_df, method='schimidt'):
        # å›ºå®šé¡ºåºçš„æ–½å¯†ç‰¹æ­£äº¤åŒ–
        def schimidt():

            col_name = factor_df.columns
            factors1 = factor_df.values

            R = np.zeros((factors1.shape[1], factors1.shape[1]))
            Q = np.zeros(factors1.shape)
            for k in range(0, factors1.shape[1]):
                R[k, k] = np.sqrt(np.dot(factors1[:, k], factors1[:, k]))
                Q[:, k] = factors1[:, k] / R[k, k]
                for j in range(k + 1, factors1.shape[1]):
                    R[k, j] = np.dot(Q[:, k], factors1[:, j])
                    factors1[:, j] = factors1[:, j] - R[k, j] * Q[:, k]

            Q = pd.DataFrame(Q, columns=col_name, index=factor_df.index)
            return Q

        # è§„èŒƒæ­£äº¤
        def canonial():
            factors1 = factor_df.values
            col_name = factor_df.columns
            D, U = np.linalg.eig(np.dot(factors1.T, factors1))
            S = np.dot(U, np.diag(D ** (-0.5)))

            Fhat = np.dot(factors1, S)
            Fhat = pd.DataFrame(Fhat, columns=col_name, index=factor_df.index)

            return Fhat

        # å¯¹ç§°æ­£äº¤
        def symmetry():
            col_name = factor_df.columns
            factors1 = factor_df.values
            D, U = np.linalg.eig(np.dot(factors1.T, factors1))
            S = np.dot(U, np.diag(D ** (-0.5)))

            Fhat = np.dot(factors1, S)
            Fhat = np.dot(Fhat, U.T)
            Fhat = pd.DataFrame(Fhat, columns=col_name, index=factor_df.index)

            return Fhat

        method_dict = {
            "schimidt": schimidt(),
            "canonial": canonial(),
            "symmetry": symmetry()
        }

        return method_dict[method]

    # åŠ æƒå‡å€¼
    def mean_weight(self,
                    data: pd.DataFrame = None,
                    meanType: str = 'Equal') -> [pd.Series, None]:

        if meanType == 'HalfTime':
            meanW = data.apply(lambda x: np.dot(x, self.half_time(len(x))))
        else:
            meanW = data.mean()
        return meanW

    # åŠ æƒæ ‡å‡†å·®
    def std_weight(self,
                   data: pd.DataFrame = None,
                   meanType: str = 'Equal') -> [pd.Series, None]:
        dataSub = data.dropna()

        if meanType == 'HalfTime':
            stdW = np.diag(pow(np.cov(dataSub.T, aweights=self.half_time(len(dataSub))), 0.5))
            stdW = pd.Series(stdW, name='std')
        else:
            stdW = dataSub.std()
        return stdW

    # åŠè¡°æƒé‡
    @staticmethod
    def half_time(period: int, decay: int = 2) -> List[str]:

        weight_list = [pow(2, (i - period - 1) / decay) for i in range(1, period + 1)]

        weight_1 = [i / sum(weight_list) for i in weight_list]

        return weight_1

    # è€ƒè™‘ç¼ºå¤±å€¼åŠ æƒæ³•
    def weighted(self, fact: pd.DataFrame, weight: np.array) -> pd.Series:
        """
        å¯¹éžç©ºéƒ¨åˆ†è¿›è¡ŒåŠ æƒ
        """
        weight_df = pd.DataFrame(np.repeat(weight.reshape(1, len(weight)), len(fact.index), axis=0),
                                 index=fact.index,
                                 columns=fact.columns)
        weight_df = pd.DataFrame(np.where(fact.isna(), np.nan, weight_df),
                                 index=fact.index,
                                 columns=fact.columns)
        # å¤åˆå› å­
        factWeighted = (fact * weight_df).div(weight_df.sum(axis=1), axis=0).sum(axis=1)
        return factWeighted


# æ”¶ç›Šé¢„æµ‹æ¨¡åž‹
class ReturnModel(object):

    def process(self,
                data: pd.DataFrame,
                method='EWMA',
                **kwargs) -> pd.Series(float):

        method_dict = {
            "Equal": self.equal_weight,
            "EWMA": self.EWMA,
            "TS": self.TimeSeries,
        }

        if method is None:
            return data

        res = method_dict[method](data, **kwargs)

        return res

    # ç­‰æƒ
    def equalWeight(self,
                    data: pd.DataFrame,
                    rp: int = 20,
                    **kwargs):
        """
        å› å­æ”¶ç›Šé¢„æµ‹--ç­‰æƒæ³•ï¼šè¿‡åŽ»ä¸€æ®µæ—¶é—´æ”¶ç›Šçš„ç­‰æƒå¹³å‡ä½œä¸ºä¸‹ä¸€æœŸå› å­æ”¶ç›Šçš„é¢„æµ‹
        :param data: å› å­æ”¶ç›Šåºåˆ—
        :param rp: æ»šåŠ¨å‘¨æœŸ
        :return:
        """
        fore_ret = data.rolling(rp, min_periods=1).mean().dropna()
        return fore_ret

    # æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡æ³•
    def EWMA(self,
             data: pd.DataFrame,
             alpha: float = 0.5,
             **kwargs):
        """
        pd.ewmä¸­comä¸Žalphaçš„å…³ç³»ä¸º 1 / alpha - 1 = com
        pd.ewmä¸­adjustå‚æ•°éœ€è¦è®¾ç½®ä¸ºFalse
        :param data:
        :param alpha: å½“æœŸæƒé‡ï¼Œå‰ä¸€æœŸæƒé‡ä¸º1-alpha
        :return:
        """
        fore_ret = data.ewm(com=1 / alpha - 1, adjust=False).mean()
        return fore_ret

    # æ—¶é—´åºåˆ—æ¨¡åž‹
    def TimeSeries(self,
                   data: pd.DataFrame,
                   rp: int = 20,
                   AR_q: int = 1,
                   MA_p: int = 1,
                   **kwargs):
        fore_ret = data.rolling(rp).apply(lambda x: self._ARMA(x, AR_q, MA_p))
        return fore_ret

    # TODO å¾…ç ”ç©¶
    def _ARMA(self, data: pd.Series, AR_q: int = 1, MA_p: int = 1):
        try:
            ar_ma = ARMA(data, order=(AR_q, MA_p)).fit(disp=0)
        except Exception as e:
            print(e)
            print("å°è¯•é‡‡ç”¨å…¶ä»–æ»žåŽé˜¶æ•°")
            forecast = np.nan
        else:
            forecast = ar_ma.predict()[-1]

        return forecast

    def KML(self, data: pd.DataFrame):
        pass


# é£Žé™©é¢„æµ‹æ¨¡åž‹
class RiskModel(object):

    def __init__(self):
        pass

    # å› å­åæ–¹å·®çŸ©é˜µä¼°è®¡
    def forecast_cov_fact(self,
                          fact_ret: pd.DataFrame,
                          decay: int = 2,
                          order: int = 2,
                          annual: int = 1):
        """

        :param fact_ret: å› å­æ”¶ç›Šåºåˆ—
        :param decay: æŒ‡æ•°åŠ æƒè¡°å‡ç³»æ•°
        :param order: è‡ªç›¸å…³ä¹‹åŽé˜¶æ•°
        :param annual: "å¹´åŒ–"å‚æ•°
        :return:
        """
        # æŒ‡æ•°åŠ æƒåæ–¹å·®çŸ©é˜µ
        F_Raw = self.exp_weight_cov(fact_ret, decay=decay)

        #  Newey-West adjustment
        matrix_orders = np.zeros(shape=(fact_ret.shape[1], fact_ret.shape[1]))
        for order_ in range(1, order + 1):
            w = 1 - order_ / (order + 1)
            # æ»žåŽorderé˜¶çš„è‡ªç›¸å…³åæ–¹å·®çŸ©é˜µ
            matrix_order = self.auto_cor_cov(fact_ret, order=order, decay=decay)
            matrix_orders += w * (matrix_order + matrix_order.T)

        #  Eigenvalue adjustment
        F_NW = annual * (F_Raw + matrix_orders)

        # ç‰¹å¾å€¼è°ƒæ•´
        F_Eigen = self.eigenvalue_adj(F_NW, period=120, M=100)

        # Volatility bias adjustment  TODO
        # F = self.vol_bias_adj(F_Eigen)
        F = F_Eigen
        return F

    # ç‰¹å¼‚æ€§æ”¶ç›Šåæ–¹å·®çŸ©é˜µé¢„æµ‹
    def forecast_cov_spec(self,
                          spec_ret: pd.DataFrame,
                          fact_exp: pd.DataFrame,
                          liq_mv: pd.DataFrame,
                          liq_mv_name: str = PVN.LIQ_MV.value,
                          decay: int = 2,
                          order: int = 5,
                          annual: int = 1):
        """

        :param spec_ret: ä¸ªè‚¡ç‰¹å¼‚æ€§æ”¶ç›Š
        :param fact_exp: å› å­æš´éœ²
        :param liq_mv: æµé€šå¸‚å€¼
        :param liq_mv_name: æµé€šå¸‚å€¼åç§°
        :param decay: æŒ‡æ•°åŠ æƒè¡°å‡å‘¨æœŸ
        :param order: Newey-Westè°ƒæ•´æœ€å¤§æ»žåŽé˜¶æ•°
        :param annual: è°ƒä»“æœŸï¼šå¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œ"å¹´åŒ–"è°ƒæ•´
        :return:
        """
        # åˆ é™¤æ— æ•ˆèµ„äº§
        eff_asset = spec_ret.iloc[-1, :].dropna().index
        spec_ret_eff = spec_ret[eff_asset]

        # Calculate the weighted covariance of the specific return index
        F_Raw = self.exp_weight_cov(spec_ret_eff, decay=decay)

        #  Newey-West adjustment: è‡ªç”±åº¦è®¾ä¸ºn-1
        matrix_orders = np.zeros(shape=(spec_ret_eff.shape[1], spec_ret_eff.shape[1]))
        for order_ in range(1, order + 1):
            w = 1 - order_ / (order + 1)
            matrix_order = self.auto_cor_cov(spec_ret_eff, order=order_, decay=decay)
            matrix_orders += w * (matrix_order + matrix_order.T)

        #  Eigenvalue adjustment
        F_NW = annual * (F_Raw + matrix_orders)

        #  Structural adjustment
        F_STR = self.structural_adj(F_NW, spec_ret_eff, fact_exp, liq_mv.iloc[:, 0], liq_mv_name)

        # Bayesian compression adjustment
        F_SH = self.Bayesian_compression(F_STR, liq_mv.iloc[:, 0], liq_mv_name)

        # æ³¢åŠ¨çŽ‡åè¯¯è°ƒæ•´  TODO

        # éžå¯¹è§’çŸ©é˜µæ›¿æ¢ä¸º0

        return F_SH

    # æŒ‡æ•°åŠ æƒåæ–¹å·®çŸ©é˜µè®¡ç®—
    def exp_weight_cov(self,
                       data: pd.DataFrame,
                       decay: int = 2) -> pd.DataFrame:
        # Exponentially weighted index volatility: Half-Life attenuation

        w_list = self.Half_time(period=data.shape[0], decay=decay)
        w_list = sorted(w_list, reverse=False)  # å‡åºæŽ’åˆ—

        cov_w = pd.DataFrame(np.cov(data.T, aweights=w_list), index=data.columns, columns=data.columns)

        return cov_w

    # è‡ªç›¸å…³åæ–¹å·®çŸ©é˜µ
    def auto_cor_cov(self,
                     data: pd.DataFrame,
                     order: int = 2,
                     decay: int = 2) -> pd.DataFrame:
        """
        çŸ©é˜µä¸ŽçŸ©é˜µç›¸å…³æ€§è®¡ç®—ï¼š
        A = np.array([[a11,a21],[a12,a22]])
        B = np.array([[b11,b21],[b12,b22]])

        matrix = [[cov([a11,a21], [a11,a21]), cov([a11,a21], [a12,a22]), cov([a11,a21], [b11,b21]), cov([a11,a21], [b12,b22])],
                  [cov([a12,a22], [a11,a21]), cov([a12,a22], [a12,a22]), cov([a12,a22], [b11,b21]), cov([a12,a22], [b12,b22])],
                  [cov([b11,b21], [a11,a21]), cov([b11,b21], [a12,a22]), cov([b11,b21], [b11,b21]), cov([b11,b21], [b12,b22])],
                  [cov([b12,b22], [a11,a21]), cov([b12,b22], [a12,a22]), cov([b12,b22], [b11,b21]), cov([b12,b22], [b12,b22])]]

        è‡ªç›¸å…³åæ–¹å·®çŸ©é˜µä¸º:
        matrix_at_cor_cov = [[cov([a11,a21], [b11,b21]), cov([a11,a21], [b12,b22])],
                             [cov([a12,a22], [b11,b21]), cov([a12,a22], [b12,b22])]

        æ³¨ï¼š
        è¾“å…¥pd.DataFrameæ ¼å¼çš„æ•°æ®è®¡ç®—åæ–¹å·®ä¼šä»¥è¡Œä¸ºå•ä½å‘é‡è¿›è¡Œè®¡ç®—
        è®¡ç®—å‡ºæ¥çš„åæ–¹å·®çŸ©é˜µä¸­å³ä¸Šè§’order*orderçŸ©é˜µæ‰æ˜¯è‡ªç›¸å…³çŸ©é˜µ
        åæ–¹å·®çŸ©é˜µï¼šæ¨ªå‘ä¸ºå½“æœŸä¸Žå„å› å­æ»žåŽé˜¶æ•°çš„åæ–¹å·®ï¼›çºµå‘ä¸ºæ»žåŽé˜¶æ•°ä¸Žå½“æœŸå„å› å­çš„åæ–¹å·®
        :param data:
        :param order:
        :param decay:
        :return:
        """

        # order matrix
        matrix_order = data.shift(order).dropna(axis=0, how='all')
        matrix = data.iloc[order:, :].copy(deep=True)

        w_list = self.Half_time(period=matrix.shape[0], decay=decay)
        w_list = sorted(w_list, reverse=False)  # å‡åºæŽ’åˆ—

        covs = np.cov(matrix.T, matrix_order.T, aweights=w_list)  # éœ€è¦å†æµ‹è¯•
        cov_order = pd.DataFrame(covs[: -matrix.shape[1], -matrix.shape[1]:],
                                 index=matrix.columns,
                                 columns=matrix.columns)

        return cov_order

    # ç‰¹å¾å€¼è°ƒæ•´
    def eigenvalue_adj(self,
                       data: np.array,
                       period: int = 120,
                       M: int = 3000,
                       alpha: float = 1.5):
        """

        :param data:Newey-Westè°ƒæ•´åŽçš„åæ–¹å·®çŸ©é˜µ
        :param period: è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ”¶ç›ŠæœŸæ•°
        :param M: è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¬¡æ•°
        :param alpha:
        :return:
        """

        # çŸ©é˜µå¥‡å¼‚å€¼åˆ†è§£
        e_vals, U0 = np.linalg.eig(data)

        # å¯¹è§’çŸ©é˜µ
        D0 = np.diag(e_vals)

        # è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
        eigenvalue_bias = []
        for i in range(M):
            S = np.random.randn(len(e_vals), period)  # æ¨¡æ‹Ÿçš„ç‰¹å¾ç»„åˆæ”¶ç›ŠçŽ‡çŸ©é˜µ, æ”¶ç›ŠæœŸæ•°æ€Žä¹ˆå®š TODO
            f = np.dot(U0, S)  # æ¨¡æ‹Ÿçš„æ”¶ç›ŠçŽ‡çŸ©é˜µ
            F = np.cov(f)  # æ¨¡æ‹Ÿçš„æ”¶ç›ŠçŽ‡åæ–¹å·®çŸ©é˜µ
            e_vas_S, U1 = np.linalg.eig(F)  # å¯¹æ¨¡æ‹Ÿçš„åæ–¹å·®çŸ©é˜µè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£
            D1 = np.diag(e_vas_S)  # ç”Ÿæˆæ¨¡æ‹Ÿåæ–¹å·®çŸ©é˜µç‰¹å¾å€¼çš„å¯¹è§’çŸ©é˜µ
            D1_real = np.dot(np.dot(U1.T, data), U1)

            D1_real = np.diag(np.diag(D1_real))  # è½¬åŒ–ä¸ºå¯¹è§’çŸ©é˜µ

            lam = D1_real / D1  # ç‰¹å¾å€¼åè¯¯
            eigenvalue_bias.append(lam)

        gam_ = reduce(lambda x, y: x + y, eigenvalue_bias)
        gam = (np.sqrt(gam_ / M) - 1) * alpha + 1
        gam[np.isnan(gam)] = 0

        F_Eigen = pd.DataFrame(np.dot(np.dot(U0, np.dot(gam ** 2, D0)), np.linalg.inv(U0)),
                               index=data.columns,
                               columns=data.columns)

        return F_Eigen

    # ç»“æž„åŒ–è°ƒæ•´
    def structural_adj(self,
                       cov: pd.DataFrame,
                       spec_ret: pd.DataFrame,
                       fact_exp: pd.DataFrame,
                       liq_mv: pd.DataFrame,
                       liq_mv_name: PVN.LIQ_MV.value,
                       time_window: int = 120):
        """

        :param cov: ç»Newey-Westè°ƒæ•´çš„ä¸ªè‚¡ç‰¹å¼‚æ”¶ç›ŠçŸ©é˜µ
        :param spec_ret: ä¸ªè‚¡ç‰¹å¼‚æ”¶ç›Šåºåˆ—
        :param fact_exp: å› å­æš´éœ²
        :param liq_mv: æµé€šå¸‚å€¼
        :param liq_mv_name: æµé€šå¸‚å€¼åç§°
        :param time_window: ä¸ªè‚¡ç‰¹å¼‚æ”¶ç›Šçš„æ—¶é—´çª—å£ï¼ˆåŽé¢è€ƒè™‘æ”¹ä¸ºç‰¹å¼‚æ”¶ç›Šåºåˆ—çš„é•¿åº¦ï¼‰
        :return:
        """
        # è®¡ç®—åè°ƒå‚æ•°
        h_n = spec_ret.count()  # éžç©ºæ•°é‡
        V_n = (h_n - 20 / 4) / 20 * 2  # æ•°æ®ç¼ºå¤±ç¨‹åº¦ï¼ˆå…ˆç”¨20æµ‹è¯•ï¼‰

        sigma_n = spec_ret.std().fillna(1)  # æ ·æœ¬ç­‰æƒæ ‡å‡†å·®ï¼ˆæ— æ³•è®¡ç®—çš„æ ‡å‡†å·®è®°ä¸º1ï¼‰  TODO

        sigma_n_steady = (spec_ret.quantile(.75) - spec_ret.quantile(0.25)) / 1.35  # æ ·æœ¬ç¨³å¥ä¼°è®¡æ ‡å‡†å·®

        Z_n = abs((sigma_n - sigma_n_steady) / sigma_n_steady)  # æ•°æ®è‚¥å°¾ç¨‹åº¦

        # å°†æ— é™å¤§å€¼æ›¿æ¢ä¸º0
        Z_n[np.isinf(Z_n)] = 0
        Z_n.fillna(0, inplace=True)

        left_, right_ = V_n.where(V_n > 0, 0), np.exp(1 - Z_n)

        left_, right_ = left_.where(left_ < 1, 1), right_.where(right_ < 1, 1)
        gam_n = left_ * right_  # ä¸ªè‚¡åè°ƒå‚æ•°[0,1]

        reg_data = pd.concat([np.log(sigma_n), liq_mv, gam_n, fact_exp], axis=1)
        reg_data.columns = ['sigma', liq_mv_name, 'gam_n'] + fact_exp.columns.tolist()

        ref_data_com = reg_data[reg_data['gam_n'] == 1]

        # åŠ æƒï¼ˆæµé€šå¸‚å€¼ï¼‰æœ€å°äºŒä¹˜æ³•ç”¨ä¼˜è´¨è‚¡ç¥¨ä¼°è®¡å› å­å¯¹ç‰¹å¼‚æ³¢åŠ¨çš„è´¡çŒ®å€¼
        model = sm.WLS(ref_data_com['sigma'], ref_data_com[fact_exp.columns], weights=ref_data_com['gam_n']).fit()

        # ä¸ªè‚¡ç»“æž„åŒ–ç‰¹å¼‚æ³¢åŠ¨é¢„æµ‹å€¼
        sigma_STR = pd.DataFrame(np.diag(np.exp(np.dot(fact_exp, model.params)) * 1.05),
                                 index=fact_exp.index,
                                 columns=fact_exp.index)

        # å¯¹ç‰¹å¼‚æ”¶ç›ŠçŸ©é˜µè¿›è¡Œç»“æž„åŒ–è°ƒæ•´
        F_STR = sigma_STR.mul((1 - gam_n), axis=0) + cov.mul(gam_n, axis=0)

        return F_STR

    # è´å¶æ–¯åŽ‹ç¼©
    def Bayesian_compression(self,
                             cov: pd.DataFrame,
                             liq_mv: pd.DataFrame,
                             liq_mv_name: PVN.LIQ_MV.value,
                             group_num: int = 10,
                             q: int = 1
                             ):
        """
        ðœŽ_ð‘›_ð‘†ð» = ð‘£_ð‘›*ðœŽ_ð‘› + (1 âˆ’ ð‘£_ð‘›)*ðœŽ_ð‘›^

        :param cov: ç»ç»“æž„åŒ–è°ƒæ•´çš„ç‰¹å¼‚æ”¶ç›ŠçŸ©é˜µ
        :param liq_mv: æµé€šå¸‚å€¼
        :param liq_mv_name: æµé€šå¸‚å€¼åç§°
        :param group_num: åˆ†ç»„ä¸ªæ•°
        :param q: åŽ‹ç¼©ç³»æ•°ï¼Œè¯¥ç³»æ•°è¶Šå¤§ï¼Œå…ˆéªŒé£Žé™©çŸ©é˜µæ‰€å æƒé‡è¶Šå¤§
        :return:
        """
        df_ = pd.DataFrame({"sigma_n": np.diag(cov), liq_mv_name: liq_mv})
        # æŒ‰æµé€šå¸‚å€¼åˆ†ç»„
        df_['Group'] = pd.cut(df_['sigma_n'], group_num, labels=[f'Group_{i}' for i in range(1, group_num + 1)])

        # å„ç»„ç‰¹å¼‚é£Žé™©å¸‚å€¼åŠ æƒå‡å€¼
        df_['weight'] = df_.groupby('Group', group_keys=False).apply(lambda x: x[liq_mv_name] / x[liq_mv_name].sum())
        sigma_n_weight = df_.groupby('Group').apply(lambda x: x['weight'] @ x['sigma_n'])
        sigma_n_weight.name = 'sigma_n_weight'

        df_N1 = pd.merge(df_, sigma_n_weight, left_on=['Group'], right_index=True, how='left')

        # ä¸ªè‚¡æ‰€å±žåˆ†ç»„ç‰¹å¼‚æ³¢åŠ¨çš„æ ‡å‡†å·®

        try:
            delta_n = df_N1.groupby('Group').apply(
                lambda x: np.nan if x.empty else pow(sum((x['sigma_n'] - x['sigma_n_weight']) ** 2) / x.shape[0], 0.5))
        except Exception as e:
            delta_n = df_N1.groupby('Group').apply(
                lambda x: np.nan if x.empty else pow(sum((x['sigma_n'] - x['sigma_n_weight']) ** 2) / x.shape[0], 0.5))
            print(e)

        delta_n.name = 'delta'

        df_N2 = pd.merge(df_N1, delta_n, left_on=['Group'], right_index=True, how='left')

        # åŽ‹ç¼©ç³»æ•°
        df_N2['V_n'] = q * abs(df_N2['sigma_n'] - df_N2['sigma_n_weight']) / (
                df_N2['delta'] + q * abs(df_N2['sigma_n'] - df_N2['sigma_n_weight']))

        # è°ƒæ•´åŽçš„ç‰¹å¼‚æ³¢åŠ¨
        sigma_SH = df_N2['V_n'] * df_N2['sigma_n_weight'] + (1 - df_N2['V_n']) * df_N2['sigma_n']
        F_SH = pd.DataFrame(np.diag(np.array(sigma_SH)), index=sigma_SH.index, columns=sigma_SH.index)

        return F_SH

    # åŠè¡°æƒé‡
    @staticmethod
    def Half_time(period: int, decay: int = 2) -> list:

        weight_list = [pow(2, (i - period - 1) / decay) for i in range(1, period + 1)]

        weight_1 = [i / sum(weight_list) for i in weight_list]

        return weight_1
